\section {Methodology}
Documents were prepared by being parsed and stored. While scripture passages are in themselves considered the largest unit of text in primary documents, the largest considered unit of text in our secondary documents was a column of a page (up to half the page). Gold alignments are contained somewhere within this column while baseline alignments are placed at the end of the page.

The reason that scripture passages are considered to be primary documents is simple: they pre-date the secondary documents by centuries. Thus all old documents (scriptural cannon in this case) are collectively grouped as \textit{primary documents} while those that we already known to contain imitations of the old documents are collected as \textit{secondary documents}.

First we run various versions of genetic algorithms over the pairings of imitated and imitator to produce initial alignments. We compute the RMSE error thereof and compare them. The versions differ from another in at least 1 aspect: word order (forward/reverse), and synonymy strictness (off/1/2/3/4/5)

Second, we feed the output of these algorithms, along with metadata such as \# of matches, \# of indels, etc., into machine learning models: (1) simple regression perceptron (SP-R), and (2) the back-propagation-trained multi-layer perceptron (BP-MLP). We compare these to determine whether machine learning was beneficial to the alignments.

Third, we feed the output of all the algorithms together in mass, into another BP-MLP to determine whether the model can outperform any of the previous models.

\subsection{Gold Standard}
\subsubsection{Primary/Imitated Documents}
Our primary source documents contained scriptural passages from the following scriptural canons:
	\begin{itemize}
		\item \textit{King James Bible},
		\item \textit{Joseph Smith Translation of the Bible},
		\item \textit{The Book of Mormon} (LDS\footnote{The Church of Jesus Christ of Latter-day Saints}),
		\item \textit{The Doctrine \& Covenants} (LDS), and
		\item \textit{The Pearl of Great Price} (LDS). 
	\end{itemize}

\subsubsection{Secondary/Imitating Documents}
As for secondary documents, our documents consisted of all discourses in the \textit{Journal of Discourses} up through volume 10. In total, there are $\JoDPagesTotal$ pages and over $\JoDCitationsTotal$ citations.

Our gold standard contains the right end of the alignments for class for each imitation instance. This, is used as the target for the regression BP-MLP. %The BP-MLP achieves lower error rates for every class and subsequently a lower error rate than any genetic algorithm alone. %This method has promise and could be extended to areas where both ends of the alignment can be used.

%Our standard also includes labels for class for each imitation instance, which allows us to interpolate, in a way, using a BP MLNN to decrease our error rates. This method has promise and could be extended to areas where both ends of the alignment can be used.

\subsubsection{Documents, Citations, and \& Alignment Format}
The alignments we have include the right end of alignments. These were hand-aligned by Dr. Galbraith using the online tool which Dr. Liddle produced. \footnote{We plan to insert scripture citation references at these positions so readers of the Journal of Discourse can benefit from them.}

Our gold standard also includes the classification of type of imitation. Our types are listed below. Future work might find that fewer or more classes exist.

	\begin{description}
		\item [Type 1] Exact Quote
		\item [Type 2] Near-exact Quote
		\item [Type 3] Fragment
		\item [Type 4] Lazy quote
		\item [Type 5] Paraphrase
		\item [Type 6] Allusion
		\item [Type 7] Other%(scattered allusion/influential citation)
	\end{description}

Baseline errors are based on the position of the last word in a column.

\subsubsection{Data Preparation: Cannonicalization}
Since many of our documents are dated before the 19th century, punctuation is often inconsistent; we therefore opt to ignore it. We do this for all documents. Furthermore, we lower-case all text. Beyond this, there is no more cannonicalization. Words were parsed based on punctuation including \textit{'s}, so graphemes such as \textit{word's} are parsed as two words: \textit{word}, \textit{s}. %Future work: except when stemming, which is only used in one of the algorithms we use.

\subsection{Algorithm Variations}
We used 8 variations of the \textit{Needleman-Wunsch} genetic algorithm. Settings that are toggled included reversing the primary documents, %stemming (using the Porter Stemmer), and 
and acceptable synset overlap levels. Genetic action costs were held fixed between all runs of genetic alignment. They are as follows:

	\begin{itemize}
		\item \textit{Match}: 3
		\item \textit{Indel}: -4
		\item \textit{Replace}: -2
%		\item \textit{}: 
	\end{itemize}

In all cases, trailing indels and replaces are trimmed from alignments until the outermost matches of the alignments are reached. %We assume that deletions are artifacts of the fact that speakers/discoursants don't always stop producing text after imitating a primary document. 
The idea (or assumption\footnote{No research seems free of \textit{assumptions}.}) behind this is that the last match is more likely to be an end alignment position than a replace or indel. \footnote{Researching this further is certainly a viable topic for future work.}
The positions of these matches serve as indicators of proposed alignment positions. These are what we use we computing error metrics. The genetic actions between these matches are used to derive meta-data such as total number of matches, total indels, total replaces, and length of match.

%\subsubsection{Needleman-Wunsch}
%[More on this. Cite it, link to it.]

\subsubsection{Document Reverse}
The idea behind reversing a document is to allow it to capture paraphrase or out-of-order quoting. Our method is naive and simply reverses the order of the words, but another viable method might be to reverse the order of sentences whiel maintaining word order within them. We leave this to future work. 

%\subsubsection{Stemming}
%We use the Java 7 port of Java 6 Porter Stemmer which was available at https://github.com/~bean5/Java-Porter-Stemmer as the Java 6 implementation was buggy in Java 7.

%Stemming allows words of different tenses, but related etymology to count as matches to the genetic algorithm. 

\subsubsection{Synynomy}
Ideally, synonymy would be as easy as submitting a tuple of words as a query to a thesaurus simply to receive a boolean back. However, this cannot be done with ease, especially when dealing with documents that are older or have wide date mismatches because language often changes over time and synonyms are not exempt. The problem is exascerbated by the fact that without parts of speech or word-sense disambiguation, the correct synset is difficult to determine without using morphological cues. Intuition seems to indicate that such methods would be helpful, but we opted to ignore them and go for various methods of synonymy matching. Since we do not aim to obtain the best single variation of a genetic algorithm, we can simply have various versions of synset matching and allow higher-order machine learning (ML) models learn to ignore that which is not helpful! A further intuition is that upon imitating, an orator might be influenced by an original document in such a way that he/she uses a synonym, hypernym, or hyponym rather than a word itself. Thus having allowing for varying strictnesses of synonymy might account for various imitation types.

We have 4 versions of synset matching. One is synonymous with synset matching to be turned off. Another was built to be very strict, providing low recall at the expense of precision while the others were built to be mixes of precision and recall. Optimizing the set of synset matching methods is a topic for future research. Our levels are as follows:

\begin{description}
	\item [Level 0]: Word strings must match perfectly to be considered synonyms (i.e. synonymy is turned off)
	\item [Level 1]: xyz
	\item [Level 2]: xyz
	\item [Level 3]: xyz
\end{description}

%\vspace{5mm}
%\indent{\textit{Words are synonymous iff they share at least 3 hyopnyms and/or hypernyms.}}
%\vspace{5mm}

\textit{WordNet} (xyz Cite here.) was used to query for obtaining synsets---including hypernyms and hyponyms. The WordNet morphology boolean was set to false for all queries. 

Since part of speech tagging and word-sense disambiguation was not performed, the query for synsets always assumed words were nouns. Since this is not the case, we decrease \textit{recall} by blacklisting certain English words---particularly modal verbs. Our blacklist also included strings such as ``\&c.'' (etc.) which appeared to easily be counted as synonyms of words when in fact they were not. Our blacklist is as follows: \textit{has, can, had, might, would, should, will, shall, have....(xyz list all.)}


\subsection{Imitation Data and Meta-data}

\subsubsection{Intrinsic Meta-Data}
From each alignment, we can derive the following \textbf{data}:
	\begin{itemize}
		\item \textit{start/end position of alignment in primary/secondary documents},
		\item \textit{alignment score}
	\end{itemize}

From each alignment, we can derive the following \textbf{meta-data}:
	\begin{itemize}
		\item \textit{length of primary alignment, length of secondary alignment},
		\item \textit{number of matches/replaces/indels in alignment},
		\item \textit{length of aligment},
		\item \textit{percentage of primary document matched}, and
		\item \textit{average word length within alignment}
	\end{itemize}

Some of these are redundant in higher order models, such as in the backpropogation-trained multi-layer perceptron. For example, given the number of matches, replaces, and indels, the score can be computed. In our work, we opt to use matches/indels/score (not replaces) since we found our data for replaces to be corrupt. This would invariably cause a bit of a problem in the simple perceptron, but the BP-MLP should be able to handle it if there are sufficient hidden nodes and layers.

\subsubsection{Extrinsic Meta-Data}
Although we do not leverage the use of extrinsic \textbf{meta-data} in our research here, we believe it is important to note that it exists. Intuition says that such meta-data would be \textit{very} helpful in optimizing our results in future work. Extrinsic meta-data which we had available included:
	\begin{itemize}
		\item \textit{Speaker's Name}
		\item \textit{Speaker's Age}
		\item \textit{Speaker's Church Position/Assignment}
		\item \textit{Volume \#}
		\item \textit{Discourse \#}
		\item \textit{Date}
	\end{itemize}


\subsection{Metric}
Since our gold standard only contains right ends of alignments, we produce error rates based on those. This does not mean, however, that we do not use the left ends of alignments. Indeed, we could use them as inputs to the BP-MLP as it is a form of meta-data which might be helpful.

Our formula for deriving error is the root mean square error (RMSE):

\center{RMSE=$\sqrt{\frac{1}{n}\sum_{i=1}^{n}{(e_i-\overline{e_i})^2}}$}

%\subsubsection{Accuracy vs. Error}
